{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dEFEND.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPKqEwpWaZeb6Jsv+wb7o1F"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"_EvmWS4qKPGQ"},"source":["---\n","    Gabriel Graells Sol√© - gabriel.graells01@estudiant.upf.edu\n","---\n","\n","# dEFEND - Code Reproduction"]},{"cell_type":"code","metadata":{"id":"dEXxpBixNMXE"},"source":["import sys\n","import csv\n","csv.field_size_limit(sys.maxsize)\n","from collections import defaultdict\n","from random import shuffle\n","import numpy as np\n","\n","\n","import tensorflow as tf\n","from keras.preprocessing.text import Tokenizer\n","from keras.callbacks import *\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.engine.topology import Layer\n","from keras.backend import epsilon\n","from keras.models import *\n","from keras.layers import *\n","from keras.optimizers import *\n","from keras import initializers\n","from keras.metrics import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sVRX1ZaQlbhm","executionInfo":{"status":"ok","timestamp":1622285337022,"user_tz":-120,"elapsed":354,"user":{"displayName":"GABRIEL GRAELLS","photoUrl":"","userId":"06318594296163771906"}},"outputId":"f12b0d37-20c9-45ca-962c-b75bbeafabb7"},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f_cakD-0qDsx","executionInfo":{"status":"ok","timestamp":1622285535037,"user_tz":-120,"elapsed":198035,"user":{"displayName":"GABRIEL GRAELLS","photoUrl":"","userId":"06318594296163771906"}},"outputId":"d44ccbbc-cd59-4ce9-d13d-0bca11a907ef"},"source":["from google.colab import drive\n","\n","drive.mount('/content/drive/')\n","PATH = \"/content/drive/My Drive/TFG/dEFEND/\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"w2TJS2UW1HtU"},"source":["## Data Preprocessing"]},{"cell_type":"code","metadata":{"id":"jKAgOOkX1GoT"},"source":["def getData() :\n","    title_file = f'{PATH}data/politifact_title.tsv'\n","    content_file = f'{PATH}data/politifact_content.tsv'\n","    comment_file = f'{PATH}data/politifact_comment.tsv'\n","\n","    news_dic = defaultdict(dict)\n","\n","    with open(title_file) as tsvfile :\n","        reader = csv.reader(tsvfile, delimiter=str(u'\\t'))\n","        for line in reader :\n","            id = line[0]\n","            news_dic[id]['title'] = line[1]\n","   \n","    with open(content_file) as tsvfile :\n","        reader = csv.reader(tsvfile, delimiter=str(u'\\t'))\n","        next(reader, None)\n","        for line in reader :\n","            id = line[0]\n","            news_dic[id]['label'] = line[1]\n","            news_dic[id]['content'] = line[2]\n","            \n","    with open(comment_file) as tsvfile :\n","        reader = csv.reader(tsvfile, delimiter=str(u'\\t'))\n","        next(reader, None)\n","        for line in reader:\n","            id = line[0]\n","            news_dic[id]['comments'] = line[1].split('::')\n","\n","    return news_dic\n","\n","def train_test_split(_data, test_size = 0.2) :\n","    id_list = list(_data.keys())\n","    shuffle(id_list)\n","    index =  len(id_list) - round(len(id_list) * test_size)\n","    \n","    train_x = []\n","    train_c = []\n","    train_y = []\n","    for i in id_list[:index]:\n","        sentences = []\n","        sentences.append(_data[i]['title'])\n","        sentences.extend(_data[i]['content'].split('. '))\n","        train_x.append(sentences)\n","        train_c.append(_data[i]['comments'])\n","        y = _data[i]['label']\n","        if y:\n","            train_y.append(np.array([0,1]))\n","        else:\n","            train_y.append(np.array([1,0]))\n","        \n","    test_x = []\n","    test_c = []\n","    test_y = []\n","    for i in id_list[index:]:       \n","        sentences = []\n","        sentences.append(_data[i]['title'])\n","        sentences.extend(_data[i]['content'].split('.')) \n","        test_x.append(sentences)\n","        test_c.append(_data[i]['comments'])\n","        y = _data[i]['label']\n","        if y:\n","            test_y.append(np.array([0,1]))\n","        else:\n","            test_y.append(np.array([1,0]))\n","\n","    train_y = np.asarray(train_y)\n","    test_y = np.asarray(test_y)\n","\n","    return train_x, train_c, train_y, test_x, test_c, test_y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eM_koDwj2YjA"},"source":["data = getData()\n","train_x, train_c, train_y, test_x, test_c, test_y = train_test_split(data, test_size = 0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VWXBBoqtKOOk"},"source":["<center>\n","    <img src=\"https://drive.google.com/file/d/17Bn0LRnE44v583ANFkEzChAGv86aPXzt/view?usp=sharing\" alt=\"net\" width=\"400\"/>\n","</center>\n","\n","\n","As seen in the paper there are two main compenents the **encoder** for words and sentences for comments and news article and, a sentence-comment **co-attention layer**.\n","\n","The **encoder** is a simple bidirectional GRU unit followed by **attention mechanism** which evaluates the relevance of words or sentences. The **attention mechanism** is implemented as a layer.\n","\n","The **co-attention layer** will be also implemented as a layer."]},{"cell_type":"markdown","metadata":{"id":"mzfH1fR1Uhga"},"source":["## Attention Layer\n","\n","Layer used for the encoding of words, sentences and comments."]},{"cell_type":"code","metadata":{"id":"g4e2wkpyPMvd"},"source":["class AttLayer(Layer) :\n","    def __init__(self, **kwargs) :\n","        super(AttLayer, self).__init__(**kwargs)\n","        self.init = initializers.get('normal')\n","        self.supports_masking = True\n","        self.attention_dim = 100\n","    \n","    def build(self, input_shape) :\n","        self.W = tf.Variable(self.init((input_shape[-1], self.attention_dim)))\n","        self.b = tf.Variable(self.init((self.attention_dim,)))\n","        self.u = tf.Variable(self.init((self.attention_dim, 1)))\n","        self._trainable_weights = [self.W, self.b, self.u]\n","        super(AttLayer, self).build(input_shape)\n","\n","    \n","    def call(self, x, mask=None):\n","        uit = tf.math.tanh(tf.tensordot(x, self.W, axes = 1) + self.b)\n","        ait = tf.tensordot(uit, self.u, axes = 1)\n","        ait = tf.squeeze(ait, -1)\n","        ait = tf.math.exp(ait)\n","        if mask is not None:\n","            # Cast the mask to floatX to avoid float64 upcasting in theano\n","            ait *= tf.cast(mask, tf.float32)\n","        \n","        ait /= tf.cast(tf.math.reduce_sum(ait, axis=1, keepdims=True) + epsilon(), tf.float32)\n","        ait = tf.expand_dims(ait, axis=-1)\n","        weighted_input = x * ait\n","        output = tf.math.reduce_sum(weighted_input, axis=1)\n","\n","        return output\n","        \n","    def compute_output_shape(self, input_shape):\n","        return (input_shape[0], input_shape[-1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jbe2xec9SzdV"},"source":["## Co-Attentino Layer"]},{"cell_type":"code","metadata":{"id":"L4uGG7R5MNs_"},"source":["class Co_attention(Layer):\n","    '''\n","        Co-Attention Layer between Senteces and Commnets\n","    '''\n","    def __init__(self, **kwargs) :\n","        self.init = initializers.get('normal')\n","        self.latent_dim = 200\n","        self.k = 80\n","        super(Co_attention, self).__init__(**kwargs)\n","    \n","    def build(self, input_shape, mask=None):\n","        self.Wl = tf.Variable(self.init((self.latent_dim, self.latent_dim)))\n","\n","        self.Wc = tf.Variable(self.init((self.k, self.latent_dim)))\n","        self.Ws =  tf.Variable(self.init((self.k, self.latent_dim)))\n","\n","        self.whs =  tf.Variable(self.init((1, self.k)))\n","        self.whc =  tf.Variable(self.init((1, self.k)))\n","        self._trainable_weights = [self.Wl, self.Wc, self.Ws, self.whs, self.whc]\n","\n","    def call(self, x, mask=None):\n","        comment_rep = x[0]\n","        sentence_rep = x[1]\n","        sentence_rep_trans = tf.transpose(sentence_rep, (0, 2, 1))\n","        comment_rep_trans = tf.transpose(comment_rep, (0, 2, 1))\n","        L = tf.math.tanh(tf.einsum('btd,dD,bDn->btn', comment_rep, self.Wl, sentence_rep_trans))\n","        L_trans = tf.transpose(L, (0, 2, 1))\n","\n","        Hs = tf.math.tanh(tf.einsum('kd,bdn->bkn', self.Ws, sentence_rep_trans) + tf.einsum('kd,bdt,btn->bkn', self.Wc, comment_rep_trans, L))\n","        Hc = tf.math.tanh(tf.einsum('kd,bdt->bkt', self.Wc, comment_rep_trans) + tf.einsum('kd,bdn,bnt->bkt', self.Ws, sentence_rep_trans, L_trans))\n","        As = tf.math.softmax(tf.einsum('yk,bkn->bn', self.whs, Hs))\n","        Ac = tf.math.softmax(tf.einsum('yk,bkt->bt', self.whc, Hc))\n","        co_s = tf.einsum('bdn,bn->bd', sentence_rep_trans, As)\n","        co_c = tf.einsum('bdt,bt->bd', comment_rep_trans, Ac)\n","        co_sc = tf.concat([co_s, co_c], axis=1)\n","\n","        return co_sc\n","\n","    def compute_output_shape(self, input_shape):\n","        return (input_shape[0][0], self.latent_dim + self.latent_dim)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TWQZyt07vvcn"},"source":["## Metrics"]},{"cell_type":"code","metadata":{"id":"jI7jX-6zvu6z"},"source":["class Metrics(Callback):\n","    def __init__(self,):\n","        self.log_file = open(f'{PATH}logs/logs.txt', 'a')\n","\n","    def on_train_begin(self, logs={}):\n","        self.val_f1s = []\n","        self.val_recalls = []\n","        self.val_precisions = []\n","        self.val_auc = []\n","        self.val_acc = []\n","\n","    \n","    def on_epoch_end(self, epoch, logs={}):\n","        val_predict_onehot = (\n","            np.asarray(self.model.predict([self.validation_data[0], self.validation_data[1]]))).round()\n","        val_targ_onehot = self.validation_data[2]\n","        val_predict = np.argmax(val_predict_onehot, axis=1)\n","        val_targ = np.argmax(val_targ_onehot, axis=1)\n","        _val_f1 = f1_score(val_targ, val_predict)\n","        _val_recall = recall_score(val_targ, val_predict)\n","        _val_precision = precision_score(val_targ, val_predict)\n","        _val_auc = roc_auc_score(val_targ, val_predict)\n","        _val_acc = accuracy_score(val_targ, val_predict)\n","        self.val_f1s.append(_val_f1)\n","        self.val_recalls.append(_val_recall)\n","        self.val_precisions.append(_val_precision)\n","        self.val_auc.append(_val_auc)\n","        self.val_acc.append(_val_acc)\n","        print(\"Epoch: %d - val_accuracy: % f - val_precision: % f - val_recall % f val_f1: %f auc: %f\" % (\n","            epoch, _val_acc, _val_precision, _val_recall, _val_f1, _val_auc))\n","        self.log_file.write(\n","            \"Epoch: %d - val_accuracy: % f - val_precision: % f - val_recall % f val_f1: %f auc: %f\\n\" % (epoch,\n","                                                                                                          _val_acc,\n","                                                                                                          _val_precision,\n","                                                                                                          _val_recall,\n","                                                                                                          _val_f1,\n","                                                                                                          _val_auc))\n","        return"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qsbZeIZ7CBz0"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"kQjxJx7FCBiW"},"source":["class dEFEND() :\n","    def __init__(self):\n","        self.model = None\n","        self.MAX_SENTENCE_LENGTH = 120\n","        self.MAX_SENTENCE_COUNT = 50\n","        self.MAX_COMS_COUNT = 150\n","        self.MAX_COMS_LENGTH = 120\n","        self.VOCABULARY_SIZE = 0\n","        self.metrics = Metrics()\n","        self.reverse_word_index = None\n","        self.word_embedding = None \n","        self.word_attention_model = None\n","        self.sentence_comment_co_model = None\n","        self.tokenizer = None\n","        self.class_count = 2\n","\n","        # Variables for calculating attention weights\n","        self.news_content_word_level_encoder = None\n","        self.comment_word_level_encoder = None\n","        self.news_content_sentence_level_encoder = None\n","        self.comment_sequence_encoder = None\n","        self.co_attention_model = None\n","\n","    def init_tokenizer(self, train_x, train_c, test_x, test_c) :\n","        self.tokenizer = Tokenizer(num_words = 20000)\n","\n","        all_sentences = []\n","        all_sentences.extend(train_x)\n","        all_sentences.extend(test_x)\n","\n","        all_comments = []\n","        all_comments.extend(train_c)\n","        all_comments.extend(test_c)\n","\n","        all_texts = []\n","        for sentence in all_sentences:\n","            all_texts.append(sentence)\n","\n","        for comments in all_comments:\n","            for comment in comments:\n","                all_texts.append(comment)\n","\n","\n","        self.tokenizer.fit_on_texts(all_texts)\n","        self.VOCABULARY_SIZE = len(self.tokenizer.word_index) + 1\n","        self.reverse_word_index = {value: key for key, value in self.tokenizer.word_index.items()}\n","\n","\n","    def encode_text(self, texts) :\n","        encoded_texts = np.zeros((len(texts), self.MAX_SENTENCE_COUNT, self.MAX_SENTENCE_LENGTH), dtype='int32')\n","        for i, text in enumerate(texts):\n","            encoded_text = np.array(pad_sequences(\n","                self.tokenizer.texts_to_sequences(text),\n","                maxlen=self.MAX_SENTENCE_LENGTH, padding='post', truncating='post', value=0))[:self.MAX_SENTENCE_COUNT]\n","            encoded_texts[i][:len(encoded_text)] = encoded_text\n","        \n","        return encoded_texts\n","\n","\n","    def encode_comments(self, comments) :\n","        encoded_texts = np.zeros((len(comments), self.MAX_COMS_COUNT, self.MAX_COMS_LENGTH), dtype='int32')\n","        for i, text in enumerate(comments):\n","            encoded_text = np.array(pad_sequences(\n","                self.tokenizer.texts_to_sequences(text),\n","                maxlen=self.MAX_COMS_LENGTH, padding='post', truncating='post', value=0))[:self.MAX_COMS_COUNT]\n","            encoded_texts[i][:len(encoded_text)] = encoded_text\n","        \n","        return encoded_texts\n","\n","\n","    def get_embedding(self, embedding_dim):\n","        embeddings_index = {}\n","        with open(f'{PATH}data/glove.6B.100d.txt') as file: \n","            for line in file:\n","                values = line.split()\n","                word = values[0]\n","                coefs = np.asarray(values[1:], dtype='float32')\n","                embeddings_index[word] = coefs\n","\n","        word_index = self.tokenizer.word_index\n","        embedding_matrix = np.random.random((len(word_index) + 1, embedding_dim))\n","        for word, i in word_index.items():\n","            embedding_vector = embeddings_index.get(word)\n","            if embedding_vector is not None:\n","                embedding_matrix[i] = embedding_vector\n","        \n","        return embedding_matrix\n","        \n","\n","    def build_model(self, n_classes=2, embedding_dim=100, aff_dim=80):\n","        embedding_matrix = self.get_embedding(embedding_dim)\n","        \n","        embedding_layer = Embedding(len( self.tokenizer.word_index) + 1,\n","                                    embedding_dim,\n","                                    weights=[embedding_matrix],\n","                                    trainable=True,\n","                                    mask_zero=False)\n","        \n","        com_embedding_layer = Embedding(len( self.tokenizer.word_index) + 1,\n","                                    embedding_dim,\n","                                    weights=[embedding_matrix],\n","                                    input_length=self.MAX_SENTENCE_LENGTH,\n","                                    trainable=True,\n","                                    mask_zero=False)\n","        \n","        # News Content Encoding\n","        # Word Level\n","        sentence_input = Input(shape=(self.MAX_SENTENCE_LENGTH,), dtype='int32')\n","        embedded_sequences = embedding_layer(sentence_input)\n","        l_lstm = Bidirectional(GRU(100, return_sequences=True), name='word_lstm')(embedded_sequences)\n","        l_att = AttLayer(name='word_attention')(l_lstm)\n","        sentEncoder = Model(sentence_input, l_att)\n","        self.news_content_word_level_encoder = sentEncoder\n","        # Sentence Level\n","        content_input = Input(shape=(self.MAX_SENTENCE_COUNT, self.MAX_SENTENCE_LENGTH), dtype='int32')\n","        content_encoder = TimeDistributed(sentEncoder)(content_input)\n","        l_lstm_sent = Bidirectional(GRU(100, return_sequences=True), name='sentence_lstm')(content_encoder)\n","        self.news_content_sentence_level_encoder = Model(content_input, l_lstm_sent)\n","\n","        # Comments Encoding\n","        comment_input = Input(shape=(self.MAX_COMS_LENGTH,), dtype='int32')\n","        com_embedded_sequences = com_embedding_layer(comment_input)\n","        c_lstm = Bidirectional(GRU(100, return_sequences=True), name='comment_lstm')(com_embedded_sequences)\n","        c_att = AttLayer(name='comment_word_attention')(c_lstm)\n","        comEncoder = Model(comment_input, c_att, name='comment_word_level_encoder')\n","        self.comment_word_level_encoder = comEncoder\n","        all_comment_input = Input(shape=(self.MAX_COMS_COUNT, self.MAX_COMS_LENGTH), dtype='int32')\n","        all_comment_encoder = TimeDistributed(comEncoder, name='comment_sequence_encoder')(all_comment_input)\n","        self.comment_sequence_encoder = Model(all_comment_input, all_comment_encoder)\n","\n","        # Co-Attention\n","        L_coattention = Co_attention(name=\"co-attention\")([all_comment_encoder, l_lstm_sent])\n","        L_Model = Model([all_comment_input, content_input], L_coattention)\n","        self.co_attention_model = L_Model\n","\n","        # Output Layer\n","        preds = Dense(2, activation='softmax')(L_coattention)\n","        model = Model(inputs=[all_comment_input, content_input], outputs=preds)\n","        model.summary()\n","\n","        # Loss \n","        optimize = RMSprop(learning_rate=0.001)\n","        model.compile(loss='categorical_crossentropy',\n","                      optimizer=optimize,\n","                      metrics = [Accuracy()])\n","        \n","        return model\n","\n","\n","    def train(self, train_x, train_y, train_c, test_c, test_x, test_y, batch_size=20, epochs=10, saved_model_file = 'model.tf') :\n","        self.init_tokenizer(train_x, train_c, test_x, test_c)\n","        self.model = self.build_model()\n","\n","        #Encode Input\n","        encoded_train_x = self.encode_text(train_x)\n","        encoded_test_x = self.encode_text(test_x)\n","        encoded_train_c = self.encode_comments(train_c)\n","        encoded_test_c = self.encode_comments(test_c)\n","    \n","        callbacks = []\n","        callbacks.append(\n","            ModelCheckpoint(\n","                filepath= f'{PATH}models/{saved_model_file}',\n","                monitor='val_loss',\n","                save_best_only=True,\n","                save_weights_only=False,\n","            )\n","        )\n","        #callbacks.append(Metrics())\n","        self.model.fit([encoded_train_c, encoded_train_x], y=train_y,\n","                       validation_data=([encoded_test_c, encoded_test_x], test_y),\n","                       batch_size=batch_size,\n","                       epochs=epochs,\n","                       verbose=1,\n","                       callbacks=callbacks)\n","        \n","\n","    def predict(self, x, c):\n","        encoded_x = self.encode_text(x)\n","        encoded_c = self.encode_comments(c)\n","        return self.model.predict([encoded_c, encoded_x])\n","\n","    def process_activation_weights(self,  encoded_text, content_word_level_attentions, sentence_co_attention):\n","        no_pad_text_att = []\n","        for k in range(len(encoded_text)):\n","            tmp_no_pad_text_att = []\n","            cur_text = encoded_text[k]\n","            for i in range(len(cur_text)):\n","                sen = cur_text[i]\n","                no_pad_sen_att = []\n","                if sum(sen) == 0:\n","                    continue\n","                for j in range(len(sen)):\n","                    wd_idx = sen[j]\n","                    if wd_idx == 0:\n","                        continue\n","                    wd = self.reverse_word_index[wd_idx]\n","                    no_pad_sen_att.append((wd, content_word_level_attentions[k][i][j]))\n","                tmp_no_pad_text_att.append((no_pad_sen_att, sentence_co_attention[k][i]))\n","            no_pad_text_att.append(tmp_no_pad_text_att)\n","\n","        # Normalize without padding tokens\n","        no_pad_text_att_normalize = None\n","        for npta in no_pad_text_att:\n","            if len(npta) == 0:\n","                continue\n","            sen_att, sen_weight = list(zip(*npta))\n","            new_sen_weight = [float(i) / sum(sen_weight) for i in sen_weight]\n","            new_sen_att = []\n","            for sw in sen_att:\n","                word_list, att_list = list(zip(*sw))\n","                att_list = [float(i) / sum(att_list) for i in att_list]\n","                new_wd_att = list(zip(word_list, att_list))\n","                new_sen_att.append(new_wd_att)\n","            no_pad_text_att_normalize = list(zip(new_sen_att, new_sen_weight))\n","\n","        return no_pad_text_att_normalize\n","\n","    def  process_activation_weights_comments(self, encoded_text, sentence_co_attention):\n","        no_pad_text_att = []\n","        for k in range(len(encoded_text)):\n","            tmp_no_pad_text_att = []\n","            cur_text = encoded_text[k]\n","            for i in range(len(cur_text)):\n","                sen = cur_text[i]\n","                no_pad_sen_att = []\n","                if sum(sen) == 0:\n","                    continue\n","                for j in range(len(sen)):\n","                    wd_idx = sen[j]\n","                    if wd_idx == 0:\n","                        continue\n","                    wd = self.reverse_word_index[wd_idx]\n","                    no_pad_sen_att.append(wd)\n","                tmp_no_pad_text_att.append((no_pad_sen_att, sentence_co_attention[k][i]))\n","\n","            no_pad_text_att.append(tmp_no_pad_text_att)\n","\n","        return no_pad_text_att\n","\n","    def activation_maps(self, news_article_sentence_list, news_article_comment_list):\n","        encoded_text = self.encode_text(news_article_sentence_list)\n","        encoded_comment = self.encode_comments(news_article_comment_list)\n","        \n","\n","        # Get weights trainned attention layer for news content\n","        content_word_level_attentions = []\n","        W, b, u = self.news_sequence_encoder.layer.get_layer('word_attention').get_weights()\n","        content_word_encoder = Model(inputs=self.news_sequence_encoder.layer.input, outputs=self.news_sequence_encoder.layer.get_layer('word_lstm').output)\n","        for sent_text in encoded_text:\n","            word_level_weights = content_word_encoder.predict(sent_text)\n","\n","            uit = np.tanh(np.matmul(word_level_weights, W) + b)\n","            ait = np.matmul(uit, u)\n","            ait = np.squeeze(ait, -1)\n","            content_word_wattention = (np.exp(ait) / np.sum(np.exp(ait), axis=1)[:, np.newaxis])\n","            content_word_level_attentions.append(content_word_wattention)\n","\n","        # Get the word level attention for comments\n","        comment_word_level_attentions = []\n","        W, b, u = self.comment_sequence_encoder.layer.get_layer('comment_word_attention').get_weights()\n","        comment_word_encoder = Model(inputs=self.comment_sequence_encoder.layer.input,\n","                                    outputs=self.comment_sequence_encoder.layer.get_layer('comment_lstm').output)\n","        for comment_text in encoded_comment:\n","            comment_word_level_weights = comment_word_encoder.predict(comment_text)\n","            uit = np.tanh(np.matmul(comment_word_level_weights, W) + b)\n","            ait = np.matmul(uit, u)\n","            ait = np.squeeze(ait, -1)\n","            comment_word_level_attention = (np.exp(ait) / np.sum(np.exp(ait), axis=1)[:, np.newaxis])\n","            comment_word_level_attentions.append(comment_word_level_attention)\n","\n","        # Get the co attention between document sentences and comments\n","        comment_level_encoder = Model(inputs=self.comment_sequence_encoder.input,\n","                                    outputs=self.comment_sequence_encoder.output)\n","        comment_level_weights = comment_level_encoder.predict(encoded_comment)\n","        sentence_level_encoder = Model(inputs=self.news_sequence_encoder.input,\n","                                    outputs=self.news_sequence_encoder.output)\n","\n","        sentence_level_weights = sentence_level_encoder.predict(encoded_text)\n","        [Wl, Wc, Ws, whs, whc] = self.co_attention_model.get_weights()\n","\n","        ### Calculate the co attention\n","        sentence_rep = sentence_level_weights\n","        comment_rep = comment_level_weights\n","        sentence_rep_trans = np.transpose(sentence_rep, axes=(0, 2, 1))\n","        comment_rep_trans = np.transpose(comment_rep, axes=(0, 2, 1))\n","\n","        L = np.tanh(np.einsum('btd,dD,bDn->btn', comment_rep, Wl, sentence_rep_trans))\n","        L_trans = np.transpose(L, axes=(0, 2, 1))\n","\n","        Hs = np.tanh(np.einsum('kd,bdn->bkn', Ws, sentence_rep_trans) + np.einsum('kd,bdt,btn->bkn', Wc, comment_rep_trans, L))\n","        Hc = np.tanh(np.einsum('kd,bdt->bkt', Wc, comment_rep_trans) + np.einsum('kd,bdn,bnt->bkt', Ws, sentence_rep_trans, L_trans))\n","        sent_unnorm_attn = np.einsum('yk,bkn->bn', whs, Hs)\n","        comment_unnorm_attn = np.einsum('yk,bkt->bt', whc, Hc)\n","        sentence_co_attention = (np.exp(sent_unnorm_attn) / np.sum(np.exp(sent_unnorm_attn), axis=1)[:, np.newaxis])\n","        comment_co_attention = (np.exp(comment_unnorm_attn) / np.sum(np.exp(comment_unnorm_attn), axis=1)[:, np.newaxis])\n","    \n","        res_comment_weight = self.process_activation_weights_comments(encoded_comment, comment_co_attention)# process_atten_weights not used !!!????\n","        res_sentence_weight = self.process_activation_weights_comments(encoded_text, sentence_co_attention)\n","\n","        return res_comment_weight, res_sentence_weight"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G-QjMc1Rmt8P","executionInfo":{"status":"ok","timestamp":1622286209850,"user_tz":-120,"elapsed":670796,"user":{"displayName":"GABRIEL GRAELLS","photoUrl":"","userId":"06318594296163771906"}},"outputId":"ae29d201-65a1-46a4-ab6e-9eb9f6cb4bd9"},"source":["dEFEND_Model = dEFEND()\n","dEFEND_Model.train(train_x, train_y, train_c, test_c, test_x, test_y, batch_size=20, epochs=10, saved_model_file = 'model.tf')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"model_4\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_2 (InputLayer)            [(None, 50, 120)]    0                                            \n","__________________________________________________________________________________________________\n","input_4 (InputLayer)            [(None, 150, 120)]   0                                            \n","__________________________________________________________________________________________________\n","time_distributed (TimeDistribut (None, 50, 200)      9693300     input_2[0][0]                    \n","__________________________________________________________________________________________________\n","comment_sequence_encoder (TimeD (None, 150, 200)     9693300     input_4[0][0]                    \n","__________________________________________________________________________________________________\n","sentence_lstm (Bidirectional)   (None, 50, 200)      181200      time_distributed[0][0]           \n","__________________________________________________________________________________________________\n","co-attention (Co_attention)     (None, 400)          72160       comment_sequence_encoder[0][0]   \n","                                                                 sentence_lstm[0][0]              \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 2)            802         co-attention[0][0]               \n","==================================================================================================\n","Total params: 19,640,762\n","Trainable params: 19,640,762\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","Epoch 1/10\n","17/17 [==============================] - 46s 667ms/step - loss: 0.1679 - accuracy: 0.0000e+00 - val_loss: 1.8454e-04 - val_accuracy: 0.0000e+00\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:absl:Found untraced functions such as gru_cell_4_layer_call_fn, gru_cell_4_layer_call_and_return_conditional_losses, gru_cell_5_layer_call_fn, gru_cell_5_layer_call_and_return_conditional_losses, gru_cell_4_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n","/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n","  category=CustomMaskWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/drive/My Drive/TFG/dEFEND/models/model.tf/assets\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/drive/My Drive/TFG/dEFEND/models/model.tf/assets\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 2/10\n","17/17 [==============================] - 7s 434ms/step - loss: 1.7352e-04 - accuracy: 0.0000e+00 - val_loss: 9.2850e-05 - val_accuracy: 0.0000e+00\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:absl:Found untraced functions such as gru_cell_4_layer_call_fn, gru_cell_4_layer_call_and_return_conditional_losses, gru_cell_5_layer_call_fn, gru_cell_5_layer_call_and_return_conditional_losses, gru_cell_4_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n","/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n","  category=CustomMaskWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/drive/My Drive/TFG/dEFEND/models/model.tf/assets\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/drive/My Drive/TFG/dEFEND/models/model.tf/assets\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 3/10\n","17/17 [==============================] - 7s 432ms/step - loss: 8.4010e-05 - accuracy: 0.0000e+00 - val_loss: 4.1883e-05 - val_accuracy: 0.0000e+00\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:absl:Found untraced functions such as gru_cell_4_layer_call_fn, gru_cell_4_layer_call_and_return_conditional_losses, gru_cell_5_layer_call_fn, gru_cell_5_layer_call_and_return_conditional_losses, gru_cell_4_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n","/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n","  category=CustomMaskWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/drive/My Drive/TFG/dEFEND/models/model.tf/assets\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/drive/My Drive/TFG/dEFEND/models/model.tf/assets\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 4/10\n","17/17 [==============================] - 7s 433ms/step - loss: 3.6898e-05 - accuracy: 0.0000e+00 - val_loss: 1.7910e-05 - val_accuracy: 0.0000e+00\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:absl:Found untraced functions such as gru_cell_4_layer_call_fn, gru_cell_4_layer_call_and_return_conditional_losses, gru_cell_5_layer_call_fn, gru_cell_5_layer_call_and_return_conditional_losses, gru_cell_4_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n","/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n","  category=CustomMaskWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/drive/My Drive/TFG/dEFEND/models/model.tf/assets\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/drive/My Drive/TFG/dEFEND/models/model.tf/assets\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 5/10\n","17/17 [==============================] - 7s 434ms/step - loss: 1.6121e-05 - accuracy: 0.0000e+00 - val_loss: 7.7500e-06 - val_accuracy: 0.0000e+00\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:absl:Found untraced functions such as gru_cell_4_layer_call_fn, gru_cell_4_layer_call_and_return_conditional_losses, gru_cell_5_layer_call_fn, gru_cell_5_layer_call_and_return_conditional_losses, gru_cell_4_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n","/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n","  category=CustomMaskWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/drive/My Drive/TFG/dEFEND/models/model.tf/assets\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/drive/My Drive/TFG/dEFEND/models/model.tf/assets\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 6/10\n","17/17 [==============================] - 7s 436ms/step - loss: 6.8089e-06 - accuracy: 0.0000e+00 - val_loss: 3.3106e-06 - val_accuracy: 0.0000e+00\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:absl:Found untraced functions such as gru_cell_4_layer_call_fn, gru_cell_4_layer_call_and_return_conditional_losses, gru_cell_5_layer_call_fn, gru_cell_5_layer_call_and_return_conditional_losses, gru_cell_4_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n","/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n","  category=CustomMaskWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/drive/My Drive/TFG/dEFEND/models/model.tf/assets\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/drive/My Drive/TFG/dEFEND/models/model.tf/assets\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 7/10\n","17/17 [==============================] - 7s 434ms/step - loss: 2.9946e-06 - accuracy: 0.0000e+00 - val_loss: 1.4463e-06 - val_accuracy: 0.0000e+00\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:absl:Found untraced functions such as gru_cell_4_layer_call_fn, gru_cell_4_layer_call_and_return_conditional_losses, gru_cell_5_layer_call_fn, gru_cell_5_layer_call_and_return_conditional_losses, gru_cell_4_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n","/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n","  category=CustomMaskWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/drive/My Drive/TFG/dEFEND/models/model.tf/assets\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/drive/My Drive/TFG/dEFEND/models/model.tf/assets\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 8/10\n","17/17 [==============================] - 7s 432ms/step - loss: 1.2848e-06 - accuracy: 0.0000e+00 - val_loss: 6.0610e-07 - val_accuracy: 0.0000e+00\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:absl:Found untraced functions such as gru_cell_4_layer_call_fn, gru_cell_4_layer_call_and_return_conditional_losses, gru_cell_5_layer_call_fn, gru_cell_5_layer_call_and_return_conditional_losses, gru_cell_4_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n","/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n","  category=CustomMaskWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/drive/My Drive/TFG/dEFEND/models/model.tf/assets\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/drive/My Drive/TFG/dEFEND/models/model.tf/assets\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 9/10\n","17/17 [==============================] - 7s 432ms/step - loss: 5.8145e-07 - accuracy: 0.0000e+00 - val_loss: 2.4847e-07 - val_accuracy: 0.0000e+00\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:absl:Found untraced functions such as gru_cell_4_layer_call_fn, gru_cell_4_layer_call_and_return_conditional_losses, gru_cell_5_layer_call_fn, gru_cell_5_layer_call_and_return_conditional_losses, gru_cell_4_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n","/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n","  category=CustomMaskWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/drive/My Drive/TFG/dEFEND/models/model.tf/assets\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/drive/My Drive/TFG/dEFEND/models/model.tf/assets\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 10/10\n","17/17 [==============================] - 7s 438ms/step - loss: 2.5826e-07 - accuracy: 0.0000e+00 - val_loss: 1.2065e-07 - val_accuracy: 0.0000e+00\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:absl:Found untraced functions such as gru_cell_4_layer_call_fn, gru_cell_4_layer_call_and_return_conditional_losses, gru_cell_5_layer_call_fn, gru_cell_5_layer_call_and_return_conditional_losses, gru_cell_4_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n","/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n","  category=CustomMaskWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/drive/My Drive/TFG/dEFEND/models/model.tf/assets\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/drive/My Drive/TFG/dEFEND/models/model.tf/assets\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"_9P2-n1mvZaE"},"source":[""],"execution_count":null,"outputs":[]}]}