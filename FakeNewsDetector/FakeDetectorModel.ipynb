{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3.8.2 64-bit","metadata":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2-final"},"toc-autonumbering":false,"toc-showcode":false,"toc-showmarkdowntxt":false,"colab":{"name":"FakeDetectorModel.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"ztW1gPjGOmfo"},"source":["---\n","    Gabriel Graells Solé - gabriel.graells01@estudiant.upf.edu\n","---\n","# Fake News Detector"]},{"cell_type":"code","metadata":{"id":"t3lpU_S2Omf9"},"source":["import pandas as pd\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","import numpy as np\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","from collections import defaultdict\n","\n","import string\n","import nltk\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xr9fD2pMOmgC"},"source":["FILENAME = \"PolitifactDataset.csv\"\n","data = pd.read_csv(f\"data/{FILENAME}\", index_col = 0)\n","\n","data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WXFuZIgsOmgE"},"source":["First we have to tranform **Tags** column to be used by one-hot-enconding preprocessing layer. To do so first we will retrive all unique tags. Since many tags are formed by two words we will concatenate both words into a single one."]},{"cell_type":"code","metadata":{"id":"Og0-4B94OmgE"},"source":["def tags_preprocessing(tags):\n","    tokens = tags.split(\",\")[:-1]\n","    tokens = [t.strip() for t in tokens]\n","    tokens = [t.replace(\" \",\"\") for t in tokens]\n","    tags_processed = \" \".join(tokens)\n","    return tags_processed\n","\n","data[\"Tags\"] = data[\"Tags\"].apply(lambda x: tags_preprocessing(x))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vMpDEL5QOmgE"},"source":["Simplify Target column to true or false and change **rating column to targer and map it into a numerical value.**\n","\n","| Category  | Label |\n","|-------|---|\n","| True | 0 |\n","| False | 1 |"]},{"cell_type":"code","metadata":{"id":"RSHnpNGrOmgF"},"source":["true = ['true','mostly-true','half-true']\n","false = ['false','barely-true','pants-fire']\n","\n","def target_preprocessing(target):\n","    if target in true:\n","        return 0\n","    elif target in false:\n","        return 1\n","\n","data = data.rename(columns = {\"Rating\": \"Target\"})\n","data[\"Target\"] = data[\"Target\"].apply(lambda x: target_preprocessing(x))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9xXSkFxOOmgG"},"source":["**Author** column will be one-hot-encoded. Since it contains authors with name and surname we will concatenate them to form one single term."]},{"cell_type":"code","metadata":{"id":"0yCTNpCzOmgG"},"source":["def author_preprocessing(author):\n","    author_processed = author.replace(\" \", \"\")\n","    return author_processed\n","\n","data[\"Author\"] = data[\"Author\"].apply(lambda x: author_preprocessing(x))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RetawN3KOmgG"},"source":["Saving the final preprocesed data."]},{"cell_type":"code","metadata":{"id":"YGn_njN0OmgH","outputId":"9825e488-d207-43ce-ca47-68c271d29ebb"},"source":["data.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Title</th>\n","      <th>Tags</th>\n","      <th>Author</th>\n","      <th>Target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Says Ron Johnson referred to \"The Lego Movie\" ...</td>\n","      <td>Corporations SmallBusiness Wisconsin RussFeingold</td>\n","      <td>RussFeingold</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>\"Forty percent of the Fortune 500 were started...</td>\n","      <td>Immigration National Economy SteveCase</td>\n","      <td>SteveCase</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>\"United States of America is twenty-sixth in s...</td>\n","      <td>NewJersey Education JimWhelan</td>\n","      <td>JimWhelan</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Says Virginia Gov. Ralph Northam said, ‘You wi...</td>\n","      <td>Fakenews FacebookFact-checks Guns Facebookposts</td>\n","      <td>Facebookposts</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Figures for September 2014’s job growth in Wis...</td>\n","      <td>Jobs Wisconsin ScottWalker</td>\n","      <td>ScottWalker</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               Title  \\\n","0  Says Ron Johnson referred to \"The Lego Movie\" ...   \n","1  \"Forty percent of the Fortune 500 were started...   \n","2  \"United States of America is twenty-sixth in s...   \n","3  Says Virginia Gov. Ralph Northam said, ‘You wi...   \n","4  Figures for September 2014’s job growth in Wis...   \n","\n","                                                Tags         Author  Target  \n","0  Corporations SmallBusiness Wisconsin RussFeingold   RussFeingold       0  \n","1             Immigration National Economy SteveCase      SteveCase       0  \n","2                      NewJersey Education JimWhelan      JimWhelan       0  \n","3    Fakenews FacebookFact-checks Guns Facebookposts  Facebookposts       1  \n","4                         Jobs Wisconsin ScottWalker    ScottWalker       0  "]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"jZ02rKG_OmgH"},"source":["data.to_csv(\"data/data.csv\", index = False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UDeH-KMwOmgH"},"source":["## From Pandas dataframe to Tensorflow dataset"]},{"cell_type":"code","metadata":{"id":"AJ2h-MdLOmgH"},"source":["# Read dataset\n","data = pd.read_csv(\"data/data.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YBaMmj9ROmgI"},"source":["def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n","    dataframe = dataframe.copy()\n","    labels = dataframe.pop('Target')\n","    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n","    if shuffle:\n","        ds = ds.shuffle(buffer_size=len(dataframe))\n","    ds = ds.batch(batch_size)\n","    return ds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m5vvchi6OmgI"},"source":["#Split\n","train, test = train_test_split(data, test_size=0.2)\n","train, val = train_test_split(data, test_size=0.2)\n","\n","#Create datasets for testing\n","batch_size = 4\n","\n","train_ds = df_to_dataset(train, batch_size=batch_size)\n","val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n","test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vN4E3cxvOmgJ"},"source":["############################################################\n","[(train_features, label_batch)] = train_ds.take(1)\n","############################################################"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IPRxHSISOmgJ"},"source":["# Preprocessing\n","\n","https://www.tensorflow.org/guide/keras/preprocessing_layers"]},{"cell_type":"code","metadata":{"id":"27eJBQyTOmgJ"},"source":["######################################################\n","type_col = train_features['Author']\n","layer = get_one_hot_author(train_ds)\n","(layer(type_col).shape)\n","######################################################"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-m0wfqo4OmgK"},"source":["## Author Preprocessing - One hot Encoding"]},{"cell_type":"code","metadata":{"id":"vTQMiO7COmgK"},"source":["def get_one_hot_author(dataset):\n","    index = preprocessing.StringLookup()\n","    feature_ds = dataset.map(lambda x, y: x['Author'])\n","    index.adapt(feature_ds)\n","    \n","    encoder = preprocessing.CategoryEncoding(max_tokens=index.vocab_size())\n","    feature_ds = feature_ds.map(index)\n","    encoder.adapt(feature_ds)\n","\n","    return lambda feature: encoder(index(feature))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lZRDEvYVOmgK"},"source":["## Tags Preprocessing - One hot Encoding"]},{"cell_type":"code","metadata":{"id":"aw6RrkOvOmgK"},"source":["def get_one_hot_tags(dataset):\n","    encoder = preprocessing.TextVectorization(output_mode = \"binary\")\n","    feature_ds = dataset.map(lambda x, y: x[\"Tags\"])\n","    encoder.adapt(feature_ds)\n","    \n","    return encoder"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Oao_e3kOOmgK"},"source":["## Title Preprocessing - One hot Encoding"]},{"cell_type":"code","metadata":{"id":"IvGqU41mOmgK"},"source":["def get_one_hot_title(dataset):\n","    encoder = preprocessing.TextVectorization(output_mode = \"binary\")\n","    feature_ds = dataset.map(lambda x, y: x[\"Title\"])\n","    encoder.adapt(feature_ds)\n","\n","    return encoder"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tsV1BvLJOmgL"},"source":["---\n","## Explicit Feature Extraction\n","\n","*\"The textual information of fake news can reveal important signals for their credibility inference. Besides some shared words used in both true and false articles (or creators/subjects), a set of frequently used words can also be extracted from the article contents, creator profiles and subject descriptions of each category respectively.\"* [1]\n","\n","[1] **FAKEDETECTOR: Effective Fake News Detection with Deep Diffusive Neural Network**\n","\n","We will split the dataset into two, True and False. For each category we will retrive the most common terms in `Title`, `Author`and `Tags`."]},{"cell_type":"code","metadata":{"id":"TTZvn3cZOmgL"},"source":["#Split\n","data_true = data[data[\"Target\"] == 0]\n","data_false = data[data[\"Target\"] == 1]    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WhE16_7AOmgL"},"source":["---\n","#### Title\n","\n","TextVectorizer layer applies a preprocessing step by applying stemming, removing stopwords, lowercasing and removing punctuation. We will apply a similar preprocessing step to extract common termns. Then we will **TF-IDF** by treating all True news and all False news as two different documents."]},{"cell_type":"code","metadata":{"id":"LtxVu129OmgL","outputId":"ea718739-1fc8-4474-85a9-3ec41cb7612c"},"source":["nltk.download(\"stopwords\")\n","\n","stemming = PorterStemmer()\n","STOPWORDS = set(stopwords.words(\"english\"))\n","\n","def getTerms(terms):\n","    terms = terms.lower() \n","    terms = terms.translate(str.maketrans(\"\",\"\", string.punctuation))\n","    terms = terms.split()\n","    terms = [t for t in terms if t not in STOPWORDS]\n","    terms = [stemming.stem(t) for t in terms]\n","    terms = ' '.join(term for term in terms)\n","    \n","    return terms"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     /Users/gabrielgraells/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"k3oTvUwTOmgM"},"source":["def get_common_words_title(data_true, data_false, VEC_DIM = 40, THRESHOLD = 0.5):\n","    #Preprocess\n","    true_titles = data_true[\"Title\"].apply(lambda x: getTerms(x))\n","    false_titles = data_false[\"Title\"].apply(lambda x: getTerms(x))\n","    \n","    true_titles = ' '.join(true_titles.values)\n","    false_titles = ' '.join(false_titles.values)\n","    \n","    corpus = [true_titles, false_titles]\n","    \n","    vectorizer = TfidfVectorizer()\n","    tf_idf = vectorizer.fit_transform(corpus)\n","    \n","    df_tfidfvect = pd.DataFrame(data = tf_idf.toarray(),index = ['True','False'],columns = vectorizer.get_feature_names()).T\n","    \n","    most_common_true_terms  = df_tfidfvect[\"True\"].sort_values(ascending = False)\n","    most_common_false_terms = df_tfidfvect[\"False\"].sort_values(ascending = False)\n","    \n","    common_terms_target = defaultdict(list)\n","    \n","    for true_term, false_term in zip(most_common_true_terms.keys(), most_common_false_terms.keys()):\n","        # Common TRUE Term  TF-IDF score\n","        true_true_val = most_common_true_terms[true_term]\n","        true_false_val = most_common_false_terms[true_term]\n","        \n","        # Common FALSE Term  TF-IDF score\n","        false_true_val = most_common_true_terms[false_term]\n","        false_false_val = most_common_false_terms[false_term]\n","           \n","        true_diff_percentage = (true_true_val-true_false_val)/(true_true_val+true_false_val)\n","        false_diff_percentage = (false_false_val-false_true_val)/(false_true_val+false_false_val)\n","        \n","        if true_diff_percentage > THRESHOLD and len(common_terms_target[\"True\"]) < VEC_DIM:\n","            common_terms_target[\"True\"].append(true_term)\n","     \n","        if false_diff_percentage > THRESHOLD and len(common_terms_target[\"False\"]) < VEC_DIM:\n","            common_terms_target[\"False\"].append(false_term)  \n","        \n","        if len(common_terms_target[\"True\"]) > VEC_DIM and len(common_terms_target[\"False\"]) > VEC_DIM:\n","            break\n","        \n","    list_common_terms = list()\n","    list_common_terms.extend(common_terms_target[\"True\"])\n","    list_common_terms.extend(common_terms_target[\"False\"])\n","    \n","    return most_common_true_terms, most_common_false_terms, list_common_terms"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nRwGy1SPOmgO"},"source":["TITLE_VEC_DIM = 40\n","most_common_true_terms, most_common_false_terms, list_common_terms = get_common_words_title(data_true, data_false, VEC_DIM = TITLE_VEC_DIM, THRESHOLD = 0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DOiAt2XQOmgO","outputId":"5ccf2540-2f5f-4871-a9d0-87f5c13620bc"},"source":["count = 0\n","for true_k, false_k in zip(most_common_true_terms.keys(), most_common_false_terms.keys()):\n","    print(f\"{true_k} - True: {most_common_true_terms[true_k]} -- False: {most_common_false_terms[true_k]}\")\n","    print(f\"{false_k} - True: {most_common_true_terms[false_k]} -- False: {most_common_false_terms[false_k]}\")\n","    print(\"---\")\n","    count += 1\n","    if count == 10:\n","        break"],"execution_count":null,"outputs":[{"output_type":"stream","text":["say - True: 0.4518524440263808 -- False: 0.6408749539617608\n","say - True: 0.4518524440263808 -- False: 0.6408749539617608\n","---\n","state - True: 0.2811871545095405 -- False: 0.186750745437761\n","state - True: 0.2811871545095405 -- False: 0.186750745437761\n","---\n","percent - True: 0.26498615823551014 -- False: 0.11390930884918986\n","obama - True: 0.11185345372755202 -- False: 0.1670813960918857\n","---\n","year - True: 0.2481193675940539 -- False: 0.16038084851252157\n","year - True: 0.2481193675940539 -- False: 0.16038084851252157\n","---\n","tax - True: 0.1948558181999815 -- False: 0.15800323485532786\n","tax - True: 0.1948558181999815 -- False: 0.15800323485532786\n","---\n","million - True: 0.13737557114554505 -- False: 0.08862014540449306\n","presid - True: 0.10830255043461386 -- False: 0.15432874102148303\n","---\n","peopl - True: 0.13049569601547736 -- False: 0.12558123043904992\n","trump - True: 0.06702329965420777 -- False: 0.12601352383126696\n","---\n","job - True: 0.12272809506217514 -- False: 0.08343262469788859\n","peopl - True: 0.13049569601547736 -- False: 0.12558123043904992\n","---\n","vote - True: 0.1174017401227679 -- False: 0.11844838946746877\n","vote - True: 0.1174017401227679 -- False: 0.11844838946746877\n","---\n","obama - True: 0.11185345372755202 -- False: 0.1670813960918857\n","percent - True: 0.26498615823551014 -- False: 0.11390930884918986\n","---\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_xQo1IZHOmgP"},"source":["---\n","#### Tags\n","\n","Similarly to `Title` we will compute the **TF-IDF** for tags for each dataset. Then we will evaluate the difference in percetange between the tf-idf score for the most common tags in each dataset. If the difference is above the threshold, then that word is stored for the particular dataset (target).\n","\n","**A list of size 2VEC_DIM is returned. The first VEC_DIM terms are for True tags and the last VEC_DIM terms for False Tags**"]},{"cell_type":"code","metadata":{"id":"YFqHZeMEOmgP"},"source":["def get_most_common_tags(data_true, data_false, VEC_DIM = 20, THRESHOLD = 0.5):\n","    true_tags = ' '.join(data_true[\"Tags\"].values)\n","    false_tags = ' '.join(data_false[\"Tags\"].values)\n","    \n","    corpus = [true_tags, false_tags]\n","    \n","    vectorizer = TfidfVectorizer()\n","    tf_idf = vectorizer.fit_transform(corpus)\n","    \n","    df_tfidfvect = pd.DataFrame(data = tf_idf.toarray(),index = ['True','False'],columns = vectorizer.get_feature_names()).T\n","    \n","    most_common_true_tags  = df_tfidfvect[\"True\"].sort_values(ascending = False)\n","    most_common_false_tags = df_tfidfvect[\"False\"].sort_values(ascending = False)\n","    \n","    common_tags_target = defaultdict(list)\n","    \n","    for true_tag, false_tag in zip(most_common_true_tags.keys(), most_common_false_tags.keys()):\n","        # Common TRUE Tag  TF-IDF score\n","        true_true_val = most_common_true_tags[true_tag]\n","        true_false_val = most_common_false_tags[true_tag]\n","        \n","        # Common FALSE Tag  TF-IDF score\n","        false_true_val = most_common_true_tags[false_tag]\n","        false_false_val = most_common_false_tags[false_tag]\n","        \n","        true_diff_percentage = (true_true_val-true_false_val)/(true_true_val+true_false_val)\n","        false_diff_percentage = (false_false_val-false_true_val)/(false_true_val+false_false_val)\n","        \n","        if true_diff_percentage > THRESHOLD and len(common_tags_target[\"True\"]) < VEC_DIM:\n","            common_tags_target[\"True\"].append(true_tag)\n","     \n","        if false_diff_percentage > THRESHOLD and len(common_tags_target[\"False\"]) < VEC_DIM:\n","            common_tags_target[\"False\"].append(false_tag)  \n","        \n","        if len(common_tags_target[\"True\"]) > VEC_DIM and len(common_tags_target[\"False\"]) > VEC_DIM:\n","            break\n","        \n","    list_common_tags = list()\n","    list_common_tags.extend(common_tags_target[\"True\"])\n","    list_common_tags.extend(common_tags_target[\"False\"])\n","            \n","    return most_common_true_tags, most_common_false_tags,list_common_tags"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DpZWN0rDOmgQ"},"source":["TAG_VEC_DIM = 20\n","most_common_true_tags, most_common_false_tags, list_common_tags = get_most_common_tags(data_true, data_false, VEC_DIM = TAG_VEC_DIM)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qYC1n1q8OmgQ","outputId":"ef9af03a-da11-4615-80a9-fa752113511a"},"source":["count = 0\n","for true_k, false_k in zip(most_common_true_tags.keys(), most_common_false_tags.keys()):\n","    print(f\"{true_k} - True: {most_common_true_tags[true_k]} -- False: {most_common_false_tags[true_k]}\")\n","    print(f\"{false_k} - True: {most_common_true_tags[false_k]} -- False: {most_common_false_tags[false_k]}\")\n","    print(\"---\")\n","    count += 1\n","    if count == 10:\n","        break"],"execution_count":null,"outputs":[{"output_type":"stream","text":["national - True: 0.6400546663625211 -- False: 0.5502402311455473\n","national - True: 0.6400546663625211 -- False: 0.5502402311455473\n","---\n","economy - True: 0.2501944312831676 -- False: 0.13152719867366897\n","checks - True: 0.03034119185917143 -- False: 0.31175207090585333\n","---\n","texas - True: 0.20660906837435783 -- False: 0.1428320157497529\n","facebookfact - True: 0.03034119185917143 -- False: 0.31175207090585333\n","---\n","healthcare - True: 0.20636826526436441 -- False: 0.20761731360884933\n","healthcare - True: 0.20636826526436441 -- False: 0.20761731360884933\n","---\n","florida - True: 0.20612746215437097 -- False: 0.1532672315122919\n","punditfact - True: 0.09078277246752087 -- False: 0.17913787059025324\n","---\n","taxes - True: 0.19480971598468005 -- False: 0.14718002231747748\n","facebookposts - True: 0.029618782529191156 -- False: 0.17783346861993587\n","---\n","wisconsin - True: 0.18951204756482473 -- False: 0.17631166632123227\n","wisconsin - True: 0.18951204756482473 -- False: 0.17631166632123227\n","---\n","education - True: 0.1632645085755415 -- False: 0.10174335368475548\n","fakenews - True: 0.006020077749835601 -- False: 0.17478986402252866\n","---\n","jobs - True: 0.15411399039579138 -- False: 0.08913413463835416\n","florida - True: 0.20612746215437097 -- False: 0.1532672315122919\n","---\n","federalbudget - True: 0.1394250006861925 -- False: 0.08978633562351286\n","taxes - True: 0.19480971598468005 -- False: 0.14718002231747748\n","---\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iGiA5bPjOmgQ"},"source":["---\n","#### Author\n","\n","An analogous procedure is applied to `Authors`."]},{"cell_type":"code","metadata":{"id":"3-esQX_9OmgQ"},"source":["def get_most_common_authors(data_true, data_false, VEC_DIM = 10, THRESHOLD = 0.5):\n","    true_authors = ' '.join(data_true[\"Author\"].values)\n","    false_authors = ' '.join(data_false[\"Author\"].values)\n","    \n","    corpus = [true_authors, false_authors]\n","    \n","    vectorizer = TfidfVectorizer()\n","    tf_idf = vectorizer.fit_transform(corpus)\n","    \n","    df_tfidfvect = pd.DataFrame(data = tf_idf.toarray(),index = ['True','False'],columns = vectorizer.get_feature_names()).T\n","\n","    most_common_true_authors  = df_tfidfvect[\"True\"].sort_values(ascending = False)\n","    most_common_false_authors = df_tfidfvect[\"False\"].sort_values(ascending = False)\n","    \n","    common_authors_target = defaultdict(list)\n","    for true_author, false_author in zip(most_common_true_authors.keys(), most_common_false_authors.keys()):\n","        # Common TRUE TF-IDF score\n","        true_true_val = most_common_true_authors[true_author]\n","        true_false_val = most_common_false_authors[true_author]\n","        \n","        # Common FALSE TF-IDF score\n","        false_true_val = most_common_true_authors[false_author]\n","        false_false_val = most_common_false_authors[false_author]\n","        \n","        true_diff_percentage = (true_true_val-true_false_val)/(true_true_val+true_false_val)\n","        false_diff_percentage = (false_false_val-false_true_val)/(false_true_val+false_false_val)\n","        \n","        if true_diff_percentage > THRESHOLD and len(common_authors_target[\"True\"]) < VEC_DIM:\n","            common_authors_target[\"True\"].append(true_author)\n","        \n","        if false_diff_percentage > THRESHOLD and len(common_authors_target[\"False\"]) < VEC_DIM:\n","            common_authors_target[\"False\"].append(false_author)\n","        \n","        if len(common_authors_target[\"True\"]) > VEC_DIM and len(common_authors_target[\"False\"]) > VEC_DIM:\n","            break\n","    \n","    list_common_authors = list()\n","    list_common_authors.extend(common_authors_target[\"True\"])\n","    list_common_authors.extend(common_authors_target[\"False\"])\n","    \n","    return most_common_true_authors, most_common_false_authors, list_common_authors"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rKnuBwJEOmgR"},"source":["AUTHOR_VEC_DIM = 10\n","most_common_true_authors, most_common_false_authors, list_common_authors = get_most_common_authors(data_true, data_false, VEC_DIM = AUTHOR_VEC_DIM)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FynqDHhPOmgR","outputId":"699e4687-f71b-4c19-f44c-831850f2363e"},"source":["count = 0\n","for true_k, false_k in zip(most_common_true_authors.keys(), most_common_false_authors.keys()):\n","    print(f\"{true_k} - True: {most_common_true_authors[true_k]} -- False: {most_common_false_authors[true_k]}\")\n","    print(f\"{false_k} - True: {most_common_true_authors[false_k]} -- False: {most_common_false_authors[false_k]}\")\n","    print(\"---\")\n","    count += 1\n","    if count == 10:\n","        break"],"execution_count":null,"outputs":[{"output_type":"stream","text":["barackobama - True: 0.6325224174156848 -- False: 0.11007405476307154\n","facebookposts - True: 0.1839249582556247 -- False: 0.6167162794259762\n","---\n","donaldtrump - True: 0.35289666787258067 -- False: 0.4787467450311673\n","donaldtrump - True: 0.35289666787258067 -- False: 0.4787467450311673\n","---\n","hillaryclinton - True: 0.29607432304563974 -- False: 0.05729882302735231\n","bloggers - True: 0.07177559346560963 -- False: 0.40410748871922153\n","---\n","berniesanders - True: 0.1869156079833584 -- False: 0.03317300280530923\n","viralimage - True: 0.04785039564373975 -- False: 0.37168841779585116\n","---\n","facebookposts - True: 0.1839249582556247 -- False: 0.6167162794259762\n","chainemail - True: 0.02841117241347048 -- False: 0.11158191852694922\n","---\n","mittromney - True: 0.1644857350253554 -- False: 0.06031455055510769\n","barackobama - True: 0.6325224174156848 -- False: 0.11007405476307154\n","---\n","scottwalker - True: 0.15850443556988794 -- False: 0.068607801256435\n","tedcruz - True: 0.07177559346560963 -- False: 0.06936173313837385\n","---\n","rickscott - True: 0.1360745626118849 -- False: 0.05428309549959692\n","scottwalker - True: 0.15850443556988794 -- False: 0.068607801256435\n","---\n","marcorubio - True: 0.1360745626118849 -- False: 0.05126736797184154\n","com - True: 0.004485974591600602 -- False: 0.06408420996480192\n","---\n","rickperry - True: 0.13457923774801805 -- False: 0.05654489114541346\n","mittromney - True: 0.1644857350253554 -- False: 0.06031455055510769\n","---\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cPBzhJKNOmgR"},"source":["---\n","# Model"]},{"cell_type":"markdown","metadata":{"id":"cEhDD4e7OmgR"},"source":["## Explicit Feature Extraction"]},{"cell_type":"code","metadata":{"id":"SOjO81SXOmgR"},"source":["class Explicit_Feature(keras.layers.Layer):\n","    def __init__(self, list_common_elements, dimension,**kwargs):\n","        super(Explicit_Feature, self).__init__(**kwargs)\n","        self.list_common_elements = list_common_elements\n","        self.dimension = dimension\n","        \n","    def build(self, input_shape):\n","        self.explicit_features = tf.Variable(initial_value=tf.zeros(shape = (input_shape[0], 2*self.dimension)), trainable=False)#input_shape[0] -> batch_size\n","        \n","    def call(self, inputs):\n","        for i, input_ in enumerate(inputs):\n","            numpy_input = input_.numpy()\n","            str_input = numpy_input.decode(\"utf-8\")\n","            tokens = str_input.split()\n","            for token in tokens:\n","                if token in self.list_common_elements:\n","                    index = self.list_common_elements.index(token)\n","                    temp = self.explicit_features[i].numpy()\n","                    temp[index] = 1\n","                    temp = temp.reshape((2*self.dimension))\n","                    self.explicit_features[i].assign(temp)\n","        \n","        return self.explicit_features"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aCViIiUKOmgS","outputId":"9d7c50a9-7fa9-4ca6-f788-9d27e74057bc"},"source":["####################################################################################################\n","# TTITLE\n","title_col = train_features['Title']\n","explicit = Explicit_Feature(list_common_elements = list_common_terms,dimension = TITLE_VEC_DIM)\n","tags_explicit_test = explicit(title_col)\n","tags_explicit_test\n","####################################################################################################"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Variable 'explicit__feature/Variable:0' shape=(4, 80) dtype=float32, numpy=\n","array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n","      dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"Gw4pNehYOmgT","outputId":"c076a30b-e871-4d02-fb3c-05f1cbf0c072"},"source":["####################################################################################################\n","# TAGS\n","tags_col = train_features['Tags']\n","test_tag = tf.convert_to_tensor([\"barackobama michigan\"], dtype=\"string\")\n","explicit = Explicit_Feature(list_common_elements = list_common_tags,dimension = TAG_VEC_DIM)\n","tags_explicit_test = explicit(test_tag)\n","tags_explicit_test\n","####################################################################################################"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Variable 'explicit__feature_1/Variable:0' shape=(1, 40) dtype=float32, numpy=\n","array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"NPz0K8OBOmgT","outputId":"e5d6e8f2-c2b7-41dd-9025-8c9fc251a199"},"source":["####################################################################################################\n","# AUTHORS\n","author_col = train_features['Author']\n","test_col = tf.convert_to_tensor([\"barackobama kellyanneconway\"], dtype=\"string\")\n","explicit = Explicit_Feature(list_common_elements = list_common_authors, dimension = AUTHOR_VEC_DIM)\n","author_implicit_test = explicit(author_col)\n","author_implicit_test\n","####################################################################################################"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Variable 'explicit__feature_2/Variable:0' shape=(4, 20) dtype=float32, numpy=\n","array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0.]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"joCc-E3cOmgU"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"Da6QuuNsOmgU"},"source":["## Latent Feature Extraction\n","\n","*Besides those explicitly visible words about the news article content, creator profile and subject description, there also exist some hidden signals about articles, creators and subjects, e.g., news article content information inconsistency and profile/description latent patterns, which can be effectively detected from the latent features as introduced in [33]. Based on such an intuition, in this paper, we propose to further extract a set of latent features for news articles, creators and subjects based on the deep recurrent neural network model.*[1]\n","\n","[1] **FAKEDETECTOR: Effective Fake News Detection with Deep Diffusive Neural Network**\n","\n","*The latent feature extraction is based on RNN model (with the basic neuron cells), which has 3 layers (1 input layer, 1 hidden layer, and 1 fusion layer)*\n","\n","* **Input Layer**: Vectorized Features\n","* **Hidden Layer**: GRU - Gated Recurrent Unit, [GRU](https://keras.io/api/layers/recurrent_layers/gru/)\n","* **Fusion Layer**: Sigmoid of weighted sum, [Dense Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)\n","\n","**About GRU tensor shape**\n","\n","It should be 3D [batch, timestep, features]. Online [answers](https://stats.stackexchange.com/questions/264546/difference-between-samples-time-steps-and-features-in-neural-network) states the following:\n","* **Samples(batch)** - This is the len(dataX), or the amount of data points you have.\n","\n","* **Time steps** - This is equivalent to the amount of time steps you run your recurrent neural network. If you want your network to have memory of 60 characters, this number should be 60.\n","\n","* **Features** - this is the amount of features in every time step. If you are processing pictures, this is the amount of pixels. In this case you seem to have 1 feature per time step."]},{"cell_type":"code","metadata":{"id":"eyh9djrMOmgU"},"source":["class Latent_Feature(keras.layers.Layer):\n","    def __init__(self, **kwargs):\n","        super(Latent_Feature, self).__init__(**kwargs)\n","        \n","    def build(self, input_shape):\n","        self.GRU = layers.GRU(32)\n","        self.dense = layers.Dense(64, activation = \"sigmoid\")      \n","        \n","    def call(self, inputs):\n","        x = self.GRU(inputs)\n","        return self.dense(x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j49gxV9pOmgU"},"source":["## Hybrid Feature Extraction Unit - HFLU"]},{"cell_type":"code","metadata":{"id":"I50QH186OmgU"},"source":["class Hybrid_Feature_Extraction(keras.layers.Layer):\n","    def __init__(self,vectorizer, list_common_elements, dimension, **kwargs):\n","        super(Hybrid_Feature_Extraction, self).__init__(**kwargs)\n","        self.list_common_elements = list_common_elements\n","        self.dimension = dimension\n","        self.vectorizer = vectorizer\n","        \n","    def build(self, input_shape):\n","        self.explicit_feature = Explicit_Feature(list_common_elements = self.list_common_elements, dimension = self.dimension)\n","        self.latent_feature = Latent_Feature()\n","        \n","    def call(self, inputs):\n","        explicit = self.explicit_feature(inputs)\n","        vectorized_inputs = self.vectorizer(inputs)\n","        vectorized_inputs = tf.reshape(vectorized_inputs, [vectorized_inputs.shape[0],1,vectorized_inputs.shape[1]])\n","        latent = self.latent_feature(vectorized_inputs)\n","        return layers.Concatenate()([explicit,latent])\n","       "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7uFJHi8EOmgU"},"source":["####################################################################################################\n","type_col = train_features['Title']\n","vectorizer_layer = get_one_hot_title(train_ds)\n","HFLU = Hybrid_Feature_Extraction(vectorizer_layer, list_common_elements = list_common_terms, dimension = TITLE_VEC_DIM)\n","y = HFLU(type_col)\n","####################################################################################################"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WJlQnUm6OmgV"},"source":["---\n","## Deep Difussive Unit - GDU\n","\n","*\"Besides the HFLU feature learning unit model, FAKEDETECTOR also uses a gated diffusive unit (GDU) model for effective relationship modeling among news articles, creators and subjects.\"*\n","\n","<center>\n","    <img src=\"https://drive.google.com/uc?id=1DlamGKtKVp9d9iM7JqVcNff-hzfyRG3v\" alt=\"gdu\" width=\"400\"/>\n","</center>\n","\n","\n","GDU can be broken down into different gates:\n","* **Forget Gate**\n","<center>\n","    <img src=\"https://drive.google.com/uc?id=1BScZ2Z3GQFCW4OgCCas-mOFdxOtFguYg\" alt=\"gdu\" alt=\"gdu\" width=\"300\"/>\n","</center>\n","\n","* **Adjust Gate**\n","<center>\n","    <img src=\"https://drive.google.com/uc?id=1nxxbQQtoEBX_SxFRXAqw9vkivZP9Zu93\" alt=\"gdu\" alt=\"gdu\" width=\"300\"/>\n","</center>\n","\n","* **Final Output**\n","<center>\n","    <img src=\"https://drive.google.com/uc?id=1k1KsvR9nBt3s0xhDOYJbxtfX08oD4qum\" alt=\"gdu\" alt=\"gdu\" width=\"300\"/>\n","</center>\n","\n","    * **Selection Gates**\n","    \n","$$ r_{i} = \\sigma (W_{r}[x_{i}^T,z_{i}^T,t_{i}^T]^T)$$\n","<center>\n","    <img src=\"https://drive.google.com/uc?id=1DUtX46xqATRJzHruR2Y2abOyllWK4Cw2\" alt=\"gdu\" alt=\"gdu\" width=\"300\"/>\n","</center>\n","\n","Where:\n","\n","$x_i$ HFLU output for `Title`.\n","\n","$z_i$ input from other GDU from `Tags`.\n","\n","$t_i$ input from other GDU from `Author`.\n","\n","\n","**!!! Size of output vector for HFLU should be equal to size of the output of GDU!!!!**"]},{"cell_type":"markdown","metadata":{"id":"0zd8Lx4HOmgV"},"source":["### Forget Gate Title"]},{"cell_type":"code","metadata":{"id":"uKoKkxOqOmgV"},"source":["\"\"\"\n","It is assumed that inputs has as first element x vector, second z vector and third t vector.\n","inputs[0] -> x\n","inputs[1] -> z\n","inputs[2] -> t\n","\"\"\"\n","class Forget_Gate_Title(keras.layers.Layer):\n","    def __init__(self):\n","        super(Forget_Gate_Title, self).__init__()\n","        \n","    def build(self, input_shape):\n","        self.shape = input_shape\n","        self.dense = layers.Dense(units = int(input_shape[1]/input_shape[0]), activation = \"sigmoid\")\n","    \n","    def call(self, inputs):\n","        f = self.dense(inputs)\n","        f = tf.reshape(f, (self.shape[1]))\n","        return layers.Multiply()([f, inputs[1]])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bv39Bp8POmgV"},"source":["### Adjust Gate Title"]},{"cell_type":"code","metadata":{"id":"Xe3wTQYXOmgV"},"source":["class Adjust_Gate_Title(keras.layers.Layer):\n","    def __init__(self):\n","        super(Adjust_Gate_Title, self).__init__()\n","        \n","    def build(self, input_shape):\n","        self.shape = input_shape\n","        self.dense = layers.Dense(units = int(input_shape[1]/input_shape[0]), activation = \"sigmoid\")\n","    \n","    def call(self, inputs):\n","        e = self.dense(inputs)\n","        e = tf.reshape(e, (self.shape[1]))\n","        return layers.Multiply()([e, inputs[2]])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R7Ik6EV2OmgV"},"source":["### GDU Title"]},{"cell_type":"code","metadata":{"id":"ukbBH9oQOmgW"},"source":["class Gated_Diffusive_Unit_Title(keras.layers.Layer):\n","    def __init__(self):\n","        super(Gated_Diffusive_Unit_Title, self).__init__()\n","\n","        \n","    def build(self, input_shape):\n","        self.units = int(input_shape[1]/input_shape[0])\n","        self.shape = input_shape\n","        self.forget_gate = Forget_Gate_Title()\n","        self.adjust_gate = Adjust_Gate_Title()\n","        self.selection_gate_r = layers.Dense(units = self.units, activation = \"sigmoid\")\n","        self.selection_gate_g = layers.Dense(units = self.units, activation = \"sigmoid\")\n","        self.dense_tanh = layers.Dense(units = self.units, activation = \"tanh\")\n","        self.ones = tf.Variable(initial_value=tf.ones(shape = (self.units)), trainable=False)\n","        \n","    def call(self, inputs):\n","        g = self.selection_gate_g(inputs)\n","        r = self.selection_gate_r(inputs)\n","        \n","        forget_output = self.forget_gate(inputs) #z\n","        adjust_output = self.adjust_gate(inputs) #t\n","        \n","        tanh_input = self.dense_tanh(inputs)\n","        \n","        input_z = layers.Concatenate()([inputs[0], forget_output, inputs[2]])\n","        input_z = tf.reshape(input_z, (self.shape[0], self.shape[1]))\n","        tanh_z = self.dense_tanh(input_z)\n","\n","        input_t = layers.Concatenate()([inputs[0], inputs[1], adjust_output])\n","        input_t = tf.reshape(input_t, (self.shape[0], self.shape[1]))\n","        tanh_t = self.dense_tanh(input_t)\n","        \n","        input_z_t = layers.Concatenate()([inputs[0], forget_output, adjust_output])\n","        input_z_t = tf.reshape(input_z_t, (self.shape[0], self.shape[1]))\n","        tanh_z_t = self.dense_tanh(input_z_t)\n","        \n","        ones_g = tf.math.subtract(self.ones, g)\n","        ones_r = tf.math.subtract(self.ones, r)\n","        \n","        output_1 = tf.math.multiply(g,r)\n","        output_1 = tf.math.multiply(output_1, tanh_z_t)\n","        \n","        output_2 = tf.math.multiply(ones_g,r)\n","        output_2 = tf.math.multiply(output_2, tanh_t)\n","        \n","        output_3 = tf.math.multiply(g, ones_r)\n","        output_3 = tf.math.multiply(output_3, tanh_z)\n","        \n","        \n","        output_4 = tf.math.multiply(ones_g, ones_r)\n","        output_4 = tf.math.multiply(output_4, tanh_input)\n","            \n","        output = tf.math.add(output_1, output_2)\n","        output = tf.math.add(output, output_3)\n","        output = tf.math.add(output, output_4)\n","        output = tf.reshape(output, (self.shape[1]))\n","        \n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"07T1IitnOmgW","outputId":"e7c8ed28-2e80-45ba-c4d1-3bd4742fec25"},"source":["########################################################################\n","x = tf.ones((2,3))\n","z = tf.ones((2,3))\n","t = tf.ones((2,3))\n","inputs = layers.Concatenate()([x,z,t])\n","inputs = tf.reshape(inputs,(3,6))\n","gated_diffusive_unit_Title = Gated_Diffusive_Unit_Title()\n","gated_diffusive_unit_Title(inputs)\n","\n","########################################################################"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(6,), dtype=float32, numpy=\n","array([-0.4780119 ,  0.2760865 , -0.44729024,  0.11744387, -0.4620368 ,\n","        0.16545723], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"markdown","metadata":{"id":"z3_3-jCyOmgW"},"source":["### Forget & Adjust Gate Author"]},{"cell_type":"code","metadata":{"id":"oVuMrhBJOmgW"},"source":["class Forget_Adjust_Gate(keras.layers.Layer):\n","    def __init__(self):\n","        super(Forget_Adjust_Gate, self).__init__()\n","    \n","    def build(self, input_shape):\n","        self.shape = input_shape\n","        self.dense = layers.Dense(units = int(input_shape[1]/input_shape[0]), activation = \"sigmoid\")\n","    \n","    def call(self, inputs):\n","        f = self.dense(inputs)\n","        f = tf.reshape(f,(self.shape[1]))\n","        return layers.Multiply()([f, inputs[1]])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S96w4_msOmgW"},"source":["### GDU Author"]},{"cell_type":"code","metadata":{"id":"9U4lQOkCOmgW"},"source":["\"\"\"\n","inputs:\n","    inputs[0] -> raw data\n","    inputs[1] -> GDU Title output\n","\"\"\"\n","class Gated_Diffusive_Unit_Author(keras.layers.Layer):\n","    def __init__(self):\n","        super(Gated_Diffusive_Unit_Author, self).__init__()\n","\n","        \n","    def build(self, input_shape):\n","        self.units = int(input_shape[1]/input_shape[0])\n","        self.shape = input_shape\n","        self.forget_gate = Forget_Adjust_Gate()\n","        self.adjust_gate = Forget_Adjust_Gate()\n","        self.selection_gate_r = layers.Dense(units = self.units, activation = \"sigmoid\")\n","        self.selection_gate_g = layers.Dense(units = self.units, activation = \"sigmoid\")\n","        self.dense_tanh = layers.Dense(units = self.units, activation = \"tanh\")\n","        self.ones = tf.Variable(initial_value=tf.ones(shape = (self.units)), trainable=False)\n","        \n","    def call(self, inputs):\n","        g = self.selection_gate_r(inputs)\n","        r = self.selection_gate_g(inputs)\n","        \n","        forget_output = self.forget_gate(inputs)#z\n","        adjust_gate = self.adjust_gate(inputs)#t\n","        \n","        tanh_input = self.dense_tanh(inputs)\n","        \n","        input_z = layers.Concatenate()([inputs[0], forget_output])\n","        input_z = tf.reshape(input_z, (self.shape[0], self.shape[1]))\n","        tanh_z = self.dense_tanh(input_z)\n","        \n","        input_t = layers.Concatenate()([inputs[0], adjust_gate])\n","        input_t = tf.reshape(input_t, (self.shape[0], self.shape[1]))\n","        tanh_t = self.dense_tanh(input_t)\n","        \n","        input_z_t = layers.Concatenate()([forget_output, adjust_gate])\n","        input_z_t = tf.reshape(input_z_t,(self.shape[0], self.shape[1]))\n","        tanh_z_t = self.dense_tanh(input_z_t)\n","        \n","        ones_g = tf.math.subtract(self.ones, g)\n","        ones_r = tf.math.subtract(self.ones, r)\n","        \n","        output_1 = tf.math.multiply(g,r)\n","        output_1 = tf.math.multiply(output_1,tanh_z_t)\n","        \n","        output_2 = tf.math.multiply(ones_g,r)\n","        output_2 = tf.math.multiply(output_2,tanh_t)\n","        \n","        output_3 = tf.math.multiply(g,ones_r)\n","        output_3 = tf.math.multiply(output_3,tanh_z)\n","        \n","        output_4 = tf.math.multiply(ones_g,ones_r)\n","        output_4 = tf.math.multiply(output_4,tanh_input)\n","        \n","        output = tf.math.multiply(output_1,output_2)\n","        output = tf.math.multiply(output,output_3)\n","        output = tf.math.multiply(output,output_4)\n","        \n","        \n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-n33i4lNOmgX","outputId":"8c2d2b79-d51b-4a21-ef73-c330e267e475"},"source":["########################################################################\n","x = tf.ones((2,3))\n","z = tf.ones((2,3))\n","inputs = layers.Concatenate()([x,z])\n","gated_diffusive_unit_Author = Gated_Diffusive_Unit_Author()\n","gated_diffusive_unit_Author(inputs)\n","########################################################################"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n","array([[3.3488273e-04, 4.7071858e-06, 2.0523685e-04],\n","       [1.6064513e-04, 2.1934145e-06, 4.3841763e-05]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":66}]},{"cell_type":"markdown","metadata":{"id":"9tH5Uu2rOmgX"},"source":[""]},{"cell_type":"code","metadata":{"id":"5E4laplMOmgX"},"source":["\"\"\"\n","inputs:\n","    inputs[0] -> Raw tensor from dataset \n","    inputs[1] -> GDU output Title \n","    inputs[2] -> GDU output Title (if exist!) \n","\"\"\"\n","\n","class Main_Layer_Author(keras.layers.Layer):\n","    def __init__(self, vectorizer, list_common_authors, dimension, name = \"Main_Layer_Author\"):\n","        super(Main_Layer_Author, self).__init__()\n","        self.vectorizer = vectorizer\n","        self.list_common_authors = list_common_authors\n","        self.dimension = dimension\n","\n","    def build(self, input_shape):\n","            self.HFLU  = Hybrid_Feature_Extraction(vectorizer, self.list_common_authors, self.dimension)\n","            self.GDU = Gated_Diffusive_Unit()\n","    \n","    def call(self, inputs):\n","        x = self.HFLU(inputs[0])\n","        x = layers.Concateante()([x,inputs[]])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gUrM2B6AOmgY"},"source":["---\n","## Deep Diffusive Network Model\n","<center>\n","    <img src=\"https://drive.google.com/uc?id=182pvb9CBDtbXDeb_qd2E28idqJ2TNEiC\" width=\"400\">\n","</center>\n","\n","All of the output vectors from GDU $h_{u,j}$, $h_{n,i}$ and $h_{s,l}$ inputed to a `softmax` layer.\n","\n","<center>\n","    <img src=\"img/softmax.png\" width=\"200\">\n","</center>\n","\n","Where:\n","\n","* $u$: `Author`\n","* $n$: `Title`\n","* $s$: `Tags`\n","\n","The used **loss function** is the `cross-entropy`for each of the individual prediction, e.i:\n","<center>\n","    <img src=\"img/losstitle.png\" width=\"250\">\n","</center>\n","\n","<center>\n","    <img src=\"img/losstag.png\" width=\"250\">\n","</center>\n","\n","<center>\n","    <img src=\"img/lossauthor.png\" width=\"250\">\n","</center>\n","\n","Finally all the individual losses are combined to form the main **objective function** represented as follows:\n","\n","<center>\n","    <img src=\"img/loss.png\" width=\"350\">\n","</center"]},{"cell_type":"code","metadata":{"id":"aDKtIRo8OmgY"},"source":["class Deep_Diffusive_Net(keras.Model):\n","    def __init__(self, name =\"DeepDiffusiveNet\"):\n","        super(Deep_Diffusive_Net, self).__init__(name = name, **kwargs)\n","        self.HFLU_title_1 = Hybrid_Feature_Extraction()#ARGUMENTS !!!\n","        self.GDU_title_1 = Gated_Diffusive_Unit()\n","        self.HFLU_title_2 = Hybrid_Feature_Extraction()#ARGUMENTS !!!\n","        self.GDU_title_2 = Gated_Diffusive_Unit()\n","        self.selfmax_title_2 = layers.Softmax()\n","        self.HFLU_title_3 = Hybrid_Feature_Extraction()#ARGUMENTS !!!\n","        self.GDU_title_3 = Gated_Diffusive_Unit()\n","        self.HFLU_title_4 = Hybrid_Feature_Extraction()#ARGUMENTS !!!\n","        self.GDU_title_4 = Gated_Diffusive_Unit()\n","        self.softmax_title = layers.Softmax()\n","        \n","        self.HFLU_tags_1 = Hybrid_Feature_Extraction()#ARGUMENTS !!!\n","        self.GDU_tags_1 = Gated_Diffusive_Unit()\n","        self.HFLU_tags_2 = Hybrid_Feature_Extraction()#ARGUMENTS !!!\n","        self.GDU_tags_2 = Gated_Diffusive_Unit()\n","        self.HFLU_tags_3= Hybrid_Feature_Extraction()#ARGUMENTS !!!\n","        self.GDU_tags_3 = Gated_Diffusive_Unit()\n","        self.selfmax_tags = layers.Softmax()\n","        \n","        self.HFLU_author_1 = Hybrid_Feature_Extraction()#ARGUMENTS !!!\n","        self.GDU_author_1 = Gated_Diffusive_Unit()\n","        self.HFLU_author_2 = Hybrid_Feature_Extraction()#ARGUMENTS !!!\n","        self.GDU_author_2 = Gated_Diffusive_Unit()\n","        self.HFLU_author_3 = Hybrid_Feature_Extraction()#ARGUMENTS !!!\n","        self.GDU_author_3 = Gated_Diffusive_Unit()\n","        self.selfmax_author = layers.Softmax()\n","    \n","    def call(self, inputs):\n","        \n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"leabibFrOmgY"},"source":["## Preprocessing Layers"]},{"cell_type":"code","metadata":{"id":"C6T0DtkeOmgY"},"source":["batch_size = 256\n","train_ds = df_to_dataset(train, batch_size=batch_size)\n","val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n","test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Funf7PIMOmgY"},"source":["all_inputs = []\n","encoded_features = []\n","\n","# Prepare Inputs\n","tags_col = keras.Input(shape=(1,), name=\"Tags\", dtype=\"string\")\n","author_col = keras.Input(shape=(1,), name=\"Author\", dtype=\"string\")\n","title_col = keras.Input(shape=(1,), name=\"Title\", dtype=\"string\")\n","all_inputs.extend([tags_col,author_col,title_col])\n","\n","#Init Preprocessing Layers\n","tags_one_hot_layer = get_one_hot_tags(train_ds)\n","authors_one_hot_layer = get_one_hot_author(train_ds)\n","title_one_hot_layer = get_one_hot_title(train_ds)\n","\n","#Encode Features\n","encoded_tags = tags_one_hot_layer(tags_col)\n","encoded_authors = authors_one_hot_layer(author_col)\n","encoded_title = title_one_hot_layer(title_col)\n","encoded_features.extend([encoded_tags, encoded_authors, encoded_title])"],"execution_count":null,"outputs":[]}]}